# 爬虫进阶

## 1 selenium的使用

### 1. selenium运行效果展示

> Selenium是一个Web的自动化测试工具，最初是为网站自动化测试而开发的，Selenium 可以直接调用浏览器，它支持所有主流的浏览器（包括PhantomJS这些无界面的浏览器），可以接收指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏等。

#### 1.1 chrome浏览器的运行

> 在下载好chromedriver以及安装好selenium模块后，执行下列代码并观察运行的过程

```python
from selenium import webdriver 

# 如果driver没有添加到了环境变量，则需要将driver的绝对路径赋值给executable_path参数
# driver = webdriver.Chrome(executable_path='/home/worker/Desktop/driver/chromedriver')
# driver = webdriver.Chrome(executable_path='/home/worker/Desktop/driver/chromedriver')

# 如果driver添加了环境变量则不需要设置executable_path
driver = webdriver.Chrome()
# webdriver.PhantomJS(executable_path='/home/worker/Desktop/driver/phantomjs') 

# 向一个url发起请求
driver.get("http://www.itcast.cn/")

# 把网页保存为图片，69版本以上的谷歌浏览器将无法使用截图功能
# driver.save_screenshot("itcast.png")

print(driver.title) # 打印页面的标题

# 退出模拟浏览器
driver.quit() # 一定要退出！不退出会有残留进程！
```



#### 1.2 phantomjs无界面浏览器的运行

> PhantomJS 是一个基于Webkit的“无界面”(headless)浏览器，它会把网站加载到内存并执行页面上的 JavaScript。下载地址：<http://phantomjs.org/download.html>



#### 1.3 无头浏览器与有头浏览器的使用场景

- 通常在开发过程中我们需要查看运行过程中的各种情况所以通常使用有头浏览器
- 在项目完成进行部署的时候，通常平台采用的系统都是服务器版的操作系统，服务器版的操作系统必须使用无头浏览器才能正常运行



### 工作原理

> 利用浏览器原生的API，封装成一套更加面向对象的Selenium WebDriver API，直接操作浏览器页面里的元素，甚至操作浏览器本身（截屏，窗口大小，启动，关闭，安装插件，配置证书之类的）

![selenium的工作原理](C:\Users\12620\Desktop\爬虫开发笔记\课件\04-selenium的使用\images\selenium的工作原理.png)

- webdriver本质是一个web-server，对外提供webapi，其中封装了浏览器的各种功能
- 不同的浏览器使用各自不同的webdriver



### selenium的安装

> 我们以谷歌浏览器的chromedriver为例

#### 3.1 在python虚拟环境中安装selenium模块

````python
pip/pip3 install selenium
````

#### 3.2 下载版本符合的webdriver

> 以chrome谷歌浏览器为例

1. 查看谷歌浏览器的版本


2. 访问https://npm.taobao.org/mirrors/chromedriver，点击进入不同版本的chromedriver下载页面



3. 点击notes.txt进入版本说明页面

   ![下载chromedriver-2](C:\Users\12620\Desktop\爬虫开发笔记\课件\04-selenium的使用\images\下载chromedriver-2.png)


4. 查看chrome和chromedriver匹配的版本

   ![下载chromedriver-3](C:\Users\12620\Desktop\爬虫开发笔记\课件\04-selenium的使用\images\下载chromedriver-3.png)


5. 根据操作系统下载正确版本的chromedriver

   ![下载chromedriver-4](C:\Users\12620\Desktop\爬虫开发笔记\课件\04-selenium的使用\images\下载chromedriver-4.png)

6. 解压压缩包后获取python代码可以调用的谷歌浏览器的webdriver可执行文件

   - windows为`chromedriver.exe`

   - linux和macos为`chromedriver`

7. chromedriver环境的配置

   - windows环境下需要将 chromedriver.exe 所在的目录设置为path环境变量中的路径
   - linux/mac环境下，将 chromedriver 所在的目录设置到系统的PATH环境值



### selenium的简单使用

> 通过代码来模拟百度搜索

```python
import time
from selenium import webdriver

# 通过指定chromedriver的路径来实例化driver对象，chromedriver放在当前目录。
# driver = webdriver.Chrome(executable_path='./chromedriver')
# chromedriver已经添加环境变量
driver = webdriver.Chrome()

# 控制浏览器访问url地址
driver.get("https://www.baidu.com/")

# 在百度搜索框中搜索'python'
driver.find_element_by_id('kw').send_keys('python')
# 点击'百度搜索'
driver.find_element_by_id('su').click()

time.sleep(6)
# 退出浏览器
driver.quit()
```

- `webdriver.Chrome(executable_path='./chromedriver')`中executable参数指定的是下载好的chromedriver文件的路径
- `driver.find_element_by_id('kw').send_keys('python')`定位id属性值是'kw'的标签，并向其中输入字符串'python'
- `driver.find_element_by_id('su').click()`定位id属性值是su的标签，并点击
  - click函数作用是：触发标签的js的click事件



## 2 图像识别引擎Tesseract

### 介绍

- Tesseract，一款由HP实验室开发由Google维护的开源OCR引擎，特点是开源，免费，支持多语言，多平台。
- 项目地址：https://github.com/tesseract-ocr/tesseract 

### Python库的安装与导入

1. 安装

```python
# PIL用于打开图片文件
pip/pip3 install pillow

# pytesseract模块用于从图片中解析数据
pip/pip3 install pytesseract
```

2. 导入

```python
from PIL import Image
import pytesseract
```



### 使用

```python
from PIL import Image
import pytesseract

# 图片路径
path = ''

# 指定可执行程序安装路径
pytesseract.pytesseract.tesseract_cmd = r'D:\Program Files\Tesseract-OCR\tesseract.exe'

img = Image.open(path)

result = pytesseract.image_to_string(img)

print(result)
```



### 图片识别引擎的使用扩展

- [tesseract简单使用与训练](https://www.cnblogs.com/cnlian/p/5765871.html)
- 其他ocr平台

```
    微软Azure 图像识别：https://azure.microsoft.com/zh-cn/services/cognitive-services/computer-vision/
    有道智云文字识别：http://aidemo.youdao.com/ocrdemo
    阿里云图文识别：https://www.aliyun.com/product/cdi/
    腾讯OCR文字识别：https://cloud.tencent.com/product/ocr
```



## 3 pillow库的使用

### 常用函数及方法

Instances of the `Image` class have the following functions:

> PIL.Image.open(*fp*, *mode='r'*)

*  Opens and identifies the given image file.
*  This is a lazy operation; this function identifies the file, but the file remains open and the actual image data is not read from the file until you try to process the data (or call the [`load()`](https://pillow-cn.readthedocs.io/zh_CN/latest/reference/Image.html#PIL.Image.Image.load) method). 



>  Image.convert(*mode=None*, *matrix=None*, *dither=None*, *palette=0*, *colors=256*)

* Returns a converted copy of this image. For the “P” mode, this method translates pixels through the palette. If mode is omitted, a mode is chosen so that all information in the image and the palette can be represented without a palette.

* The current version supports all possible conversions between “L”, “RGB” and “CMYK.” The **matrix** argument only supports “L” and “RGB”.

* Return Type: An `Image` object



> Image.point(*lut*, *mode=None*)

* Maps this image through a lookup table or function.
* `params` :
  * **lut** – A lookup table, containing 256 (or 65336 if self.mode==”I” and mode == “L”) values per band in the image. A function can be used instead, it should take a single argument. The function is called once for each possible pixel value, and the resulting table is applied to all bands of the image.
  * **mode** – Output mode (default is same as input). In the current version, this can only be used if the source image has mode “L” or “P”, and the output has mode “1” or the source image mode is “I” and the output mode is “L”.
* `Return` : An `Image` object



> Image.save(*fp*, *format=None*, ***params*)

* Saves this image under the given filename. If no format is specified, the format to use is determined from the filename extension, if possible.

  Keyword options can be used to provide additional instructions to the writer. If a writer doesn’t recognise an option, it is silently ignored. The available options are described later in this handbook.

  You can use a file object instead of a filename. In this case, you must always specify the format. The file object must implement the **seek**, **tell**, and **write** methods, and be opened in binary mode.

* `params` :

  * `file` : File name or file object.
  * `format` – Optional format override. If omitted, the format to use is determined from the filename extension. If a file object was used instead of a filename, this parameter should always be used.
  * `options` – Extra parameters to the image writer.

* `Return` : None



### 常用属性

Instances of the `Image` class have the following attributes:

> PIL.Image.format

* The file format of the source file. For images created by the library itself (via a factory function, or by running a method on an existing image), this attribute is set to `None`.

* Type :`string` or `None`

> PIL.Image.mode

* Image mode. This is a string specifying the pixel format used by the image. Typical values are “1”, “L”, “RGB”, or “CMYK.” See [*概念*](https://pillow-cn.readthedocs.io/zh_CN/latest/handbook/concepts.html) for a full list.

* Type :`string`

> PIL.Image.size

* Image size, in pixels. The size is given as a 2-tuple (width, height).

* Type :*tuple* `(width, height)`

> PIL.Image.palette

* Colour palette table, if any. If mode is “P”, this should be an instance of the [`ImagePalette`](https://pillow-cn.readthedocs.io/zh_CN/latest/reference/ImagePalette.html#PIL.ImagePalette.ImagePalette) class. Otherwise, it should be set to `None`.

* Type :[`ImagePalette`](https://pillow-cn.readthedocs.io/zh_CN/latest/reference/ImagePalette.html#PIL.ImagePalette.ImagePalette) or `None`

> PIL.Image.info

* A dictionary holding data associated with the image. This dictionary is used by file handlers to pass on various non-image information read from the file. See documentation for the various file handlers for details.
* Most methods ignore the dictionary when returning new images; since the keys are not standardized, it’s not possible for a method to know if the operation affects the dictionary. If you need the information later on, keep a reference to the info dictionary returned from the open method.
* Type : `dict`

### [Pillow 中文文档](https://pillow-cn.readthedocs.io/zh_CN/latest/index.html)



## 4 Scrapy框架

### 流程

<img src="C:\Users\12620\Desktop\爬虫开发笔记\课件\07-scrapy爬虫框架\images\1.3.3.scrapy工作流程.png" width = "140%" />



### scrapy中每个模块的具体作用

<img src="C:\Users\12620\Desktop\爬虫开发笔记\课件\07-scrapy爬虫框架\images\1.3.4.scrapy组件.png" width = "80%" /> 



### 安装

```python
pip/pip3 install scrapy
```



###  scrapy项目开发流程

1. 创建项目:

   ```python
   scrapy startproject <projectName>
   ```

2. 生成一个爬虫:

   ```python
   scrapy genspider <spiderName> <allowed_domain>
   ```

   `spiderName`: 爬虫文件名

   `allowed_doamin` : 为对于爬虫设置的爬取范围，设置之后用于过滤要爬取的url，如果爬取的url与允许的域不通则被过滤掉。

   生成的目录和文件结果如下：

   <img src="C:\Users\12620\Desktop\爬虫开发笔记\课件\07-scrapy爬虫框架\images\2.2.scrapy入门使用-2.png" width = "60%" />

3. 建模

   在items.py文件中定义要提取的字段：

   ```python
   class MyspiderItem(scrapy.Item): 
       name = scrapy.Field()   # 讲师的名字
       title = scrapy.Field()  # 讲师的职称
       desc = scrapy.Field()   # 讲师的介绍
   ```

   模板类定义以后需要在爬虫中导入并且实例化，之后的使用方法和使用字典相同

   ```python
   from myspider.items import MyspiderItem   # 导入Item，注意路径
   ...
       def parse(self, response)
   
           item = MyspiderItem() # 实例化后可直接使用
   
           item['name'] = node.xpath('./h3/text()').extract_first()
           item['title'] = node.xpath('./h4/text()').extract_first()
           item['desc'] = node.xpath('./p/text()').extract_first()
           
           print(item)
   ```

   

4. 解析响应数据

   ```python
   import scrapy
   
   class ItcastSpider(scrapy.Spider):  # 继承scrapy.spider
   	# 爬虫名字 
       name = 'itcast' 
       # 允许爬取的范围
       allowed_domains = ['itcast.cn'] 
       # 开始爬取的url地址
       start_urls = ['http://www.itcast.cn/channel/teacher.shtml']
       
       # 数据提取的方法，接受下载中间件传过来的response
       def parse(self, response): 
       	# scrapy的response对象可以直接进行xpath
       	names = response.xpath('//div[@class="tea_con"]//li/div/h3/text()') 
       	print(names)
   
       	# 获取具体数据文本的方式如下
           # 分组
       	li_list = response.xpath('//div[@class="tea_con"]//li') 
           for li in li_list:
           	# 创建一个数据字典
               item = {}
               # 利用scrapy封装好的xpath选择器定位元素，并通过extract()或extract_first()来获取结果
               item['name'] = li.xpath('.//h3/text()').extract_first() # 老师的名字
               item['level'] = li.xpath('.//h4/text()').extract_first() # 老师的级别
               item['text'] = li.xpath('.//p/text()').extract_first() # 老师的介绍
               print(item)
   ```

   **注意：**

   * scrapy.Spider爬虫类中必须有名为parse的解析
   * 如果网站结构层次比较复杂，也可以自定义其他解析函数
   * 在解析函数中提取的url地址如果要发送请求，则必须属于allowed_domains范围内，但是start_urls中的url地址不受这个限制，我们会在后续的课程中学习如何在解析函数中构造发送请求
   * 启动爬虫的时候注意启动的位置，是在项目路径下启动
   * parse()函数中使用yield返回数据，**解析函数中的yield能够传递的对象只能是：BaseItem, Request, dict, None**



4. 保存数据

> 利用管道pipeline来处理(保存)数据,在pipelines.py文件中定义对数据的操作

1. 定义一个管道类

2. 重写管道类的process_item方法

3. process_item方法处理完item之后必须返回给引擎

   ```python
   import json
   
   class ItcastPipeline():
       # 爬虫文件中提取数据的方法每yield一次item，就会运行一次
       # 该方法为固定名称函数
       def process_item(self, item, spider):
           print(item)
           return item
   ```

4. 在.py配置启用管道

   ```python
   ITEM_PIPELINES = {
   'myspider.pipelines.ItcastPipeline': 400
   }
   ```

5. 修改settings.py

   1. 可以在settings中设置ROBOTS协议

   ```python
   # False表示忽略网站的robots.txt协议，默认为True
   ROBOTSTXT_OBEY = False
   ```

   2. 可以在settings中设置User-Agent：

   ```python
   # scrapy发送的每一个请求的默认UA都是设置的这个User-Agent
   USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'
   ```

   3. 等等其他设置参数

6. 运行scrapy

```shell
cd <project_path>
scrapy crawl <spiderName>
```

### 更多

#### item在解析方法直接传递

在爬虫文件的parse方法中：

```python
......
	# 提取下一页的href
	next_url = response.xpath('//a[contains(text(),">")]/@href').extract_first()

	# 判断是否是最后一页
	if next_url != 'javascript:void(0)':

        # 构造完整url
        url = 'https://hr.163.com/position/list.do' + next_url

		# 构造scrapy.Request对象，并yield给引擎
		# 利用callback参数指定该Request对象之后获取的响应用哪个函数进行解析
    	yield scrapy.Request(url, callback=self.parse)
......
```

scrapy.Request的更多参数

```python
scrapy.Request(url[,callback,method="GET",headers,body,cookies,meta,dont_filter=False])
```

##### 参数解释

1. 中括号里的参数为可选参数
2. **callback**：表示当前的url的响应交给哪个函数去处理
3. **meta**：实现数据在不同的解析函数中传递，meta默认带有部分数据，比如下载延迟，请求深度等
4. dont_filter:默认为False，会过滤请求的url地址，即请求过的url地址不会继续被请求，对需要重复请求的url地址可以把它设置为Ture，比如贴吧的翻页请求，页面的数据总是在变化;start_urls中的地址会被反复请求，否则程序不会启动
5. method：指定POST或GET请求
6. headers：接收一个字典，其中不包括cookies
7. cookies：接收一个字典，专门放置cookies
8. body：接收json字符串，为POST的数据，发送payload_post请求时使用（在下一章节中会介绍post请求）

meta参数的使用

> meta的作用：meta可以实现数据在不同的解析函数中的传递

在爬虫文件的parse方法中，提取详情页增加之前callback指定的parse_detail函数：

```
def parse(self,response):
    ...
    yield scrapy.Request(detail_url, callback=self.parse_detail,meta={"item":item})
...

def parse_detail(self,response):
    #获取之前传入的item
    item = resposne.meta["item"]
```

##### 特别注意

1. meta参数是一个字典
2. meta字典中有一个固定的键`proxy`，表示代理ip，关于代理ip的使用我们将在scrapy的下载中间件的学习中进行介绍



### response响应对象的常用属性

- response.url：当前响应的url地址
- response.request.url：当前响应对应的请求的url地址
- response.headers：响应头
- response.requests.headers：当前响应的请求头
- response.body：响应体，也就是html代码，byte类型
- response.status：响应状态码



### response响应对象常用方法

* response.xpath('<xpath_expression>')
* reponse.css('<css_expression>')
* response.extrac_first()
* response.extrac()



### scrapy发送带cookies的请求

##### 应用场景

1. cookie过期时间很长，常见于一些不规范的网站
2. 能在cookie过期之前把所有的数据拿到
3. 配合其他程序使用，比如其使用selenium把登陆之后的cookie获取到保存到本地，scrapy发送请求之前先读取本地cookie

#### 重构scrapy的starte_rquests方法

scrapy中start_url是通过start_requests来进行处理的，其实现代码如下

```python
# 这是源代码
def start_requests(self):
    cls = self.__class__
    if method_is_overridden(cls, Spider, 'make_requests_from_url'):
        warnings.warn(
            "Spider.make_requests_from_url method is deprecated; it "
            "won't be called in future Scrapy releases. Please "
            "override Spider.start_requests method instead (see %s.%s)." % (
                cls.__module__, cls.__name__
            ),
        )
        for url in self.start_urls:
            yield self.make_requests_from_url(url)
    else:
        for url in self.start_urls:
            yield Request(url, dont_filter=True)
```

#### 修改start_requests

```python
import scrapy
import re

class Login1Spider(scrapy.Spider):
    name = 'login1'
    allowed_domains = ['github.com']
    start_urls = ['https://github.com/NoobPythoner'] # 这是一个需要登陆以后才能访问的页面

    def start_requests(self): # 重构start_requests方法
        # 这个cookies_str是抓包获取的
        cookies_str = '...' # 抓包获取
        # 将cookies_str转换为cookies_dict
        cookies_dict = {i.split('=')[0]:i.split('=')[1] for i in cookies_str.split('; ')}
        yield scrapy.Request(
            self.start_urls[0],
            callback=self.parse,
            cookies=cookies_dict
        )

    def parse(self, response): # 通过正则表达式匹配用户名来验证是否登陆成功
        # 正则匹配的是github的用户名
        result_list = re.findall(r'noobpythoner|NoobPythoner', response.body.decode()) 
        print(result_list)
        pass
```

### 发送post请求

>我们知道可以通过scrapy.Request()指定method、body参数来发送post请求；但是通常使用scrapy.FormRequest()来发送post请求

#### 3.1 发送post请求

> 注意：scrapy.FormRequest()能够发送表单和ajax请求，参考阅读 https://www.jb51.net/article/146769.htm

##### 3.1.1 思路分析

1. 找到post的url地址：点击登录按钮进行抓包，然后定位url地址为https://github.com/session

2. 找到请求体的规律：分析post请求的请求体，其中包含的参数均在前一次的响应中

3. 否登录成功：通过请求个人主页，观察是否包含用户名

##### 3.1.2 代码实现如下：

```python
import scrapy
import re

class Login2Spider(scrapy.Spider):
   name = 'login2'
   allowed_domains = ['github.com']
   start_urls = ['https://github.com/login']

   def parse(self, response):
       authenticity_token = response.xpath("//input[@name='authenticity_token']/@value").extract_first()
       utf8 = response.xpath("//input[@name='utf8']/@value").extract_first()
       commit = response.xpath("//input[@name='commit']/@value").extract_first()
        
        #构造POST请求，传递给引擎
       yield scrapy.FormRequest(
           "https://github.com/session",
           formdata={
               "authenticity_token":authenticity_token,
               "utf8":utf8,
               "commit":commit,
               "login":"noobpythoner",
               "password":"***"
           },
           callback=self.parse_login
       )

   def parse_login(self,response):
       ret = re.findall(r"noobpythoner|NoobPythoner",response.text)
       print(ret)
```

##### 小技巧

> 在settings.py中通过设置COOKIES_DEBUG=TRUE 能够在终端看到cookie的传递传递过程



### scrapy管道的使用

### 1. pipeline中常用的方法：

1. process_item(self,item,spider): 
   - 管道类中必须有的函数
   - 实现对item数据的处理
   - 必须return item
2. open_spider(self, spider): 在爬虫开启的时候仅执行一次
3. close_spider(self, spider): 在爬虫关闭的时候仅执行一次


### 2. 管道文件的修改

> 继续完善wangyi爬虫，在pipelines.py代码中完善

```python
import json
from pymongo import MongoClient

class WangyiFilePipeline(object):
    def open_spider(self, spider):  # 在爬虫开启的时候仅执行一次
        if spider.name == 'itcast':
            self.f = open('json.txt', 'a', encoding='utf-8')

    def close_spider(self, spider):  # 在爬虫关闭的时候仅执行一次
        if spider.name == 'itcast':
            self.f.close()

    def process_item(self, item, spider):
        if spider.name == 'itcast':
            self.f.write(json.dumps(dict(item), ensure_ascii=False, indent=2) + ',\n')
        # 不return的情况下，另一个权重较低的pipeline将不会获得item
        return item  

class WangyiMongoPipeline(object):
    def open_spider(self, spider):  # 在爬虫开启的时候仅执行一次
        if spider.name == 'itcast':
        # 也可以使用isinstanc函数来区分爬虫类:
            con = MongoClient(host='127.0.0.1', port=27017) # 实例化mongoclient
            self.collection = con.itcast.teachers # 创建数据库名为itcast,集合名为teachers的集合操作对象

    def process_item(self, item, spider):
        if spider.name == 'itcast':
            self.collection.insert(item) 
            # 此时item对象必须是一个字典,再插入
            # 如果此时item是BaseItem则需要先转换为字典：dict(BaseItem)
        # 不return的情况下，另一个权重较低的pipeline将不会获得item
        return item  
```


### 3. 开启管道

在settings.py设置开启pipeline

```python
......
ITEM_PIPELINES = {
    'myspider.pipelines.ItcastFilePipeline': 400, # 400表示权重
    'myspider.pipelines.ItcastMongoPipeline': 500, # 权重值越小，越优先执行！
}
......
```

**别忘了开启mongodb数据库 ```sudo service mongodb start```**
**并在mongodb数据库中查看 ```mongo```**

### 4. pipeline使用注意点

1. 使用之前需要在settings中开启
2. pipeline在setting中键表示位置(即pipeline在项目中的位置可以自定义)，值表示距离引擎的远近，越近数据会越先经过：**权重值小的优先执行**
3. 有多个pipeline的时候，process_item的方法必须return item,否则后一个pipeline取到的数据为None值
4. pipeline中process_item的方法必须有，否则item没有办法接受和处理
5. process_item方法接受item和spider，其中spider表示当前传递item过来的spider
6. open_spider(spider) :能够在爬虫开启的时候执行一次
7. close_spider(spider) :能够在爬虫关闭的时候执行一次
8. 上述俩个方法经常用于爬虫和数据库的交互，在爬虫开启的时候建立和数据库的连接，在爬虫关闭的时候断开和数据库的连接



## 5 mitmproxy

### 准备工作

#### 介绍

* mitmweb : 

  <img src="Image/mitmproxy.jpg" alt="mitmproxy" style="zoom: 50%;" />

  > mitmweb is mitmproxy’s web-based user interface that allows interactive examination and modification of HTTP traffic. Like mitmproxy, it differs from mitmdump in that all flows are kept in memory, which means that it’s intended for taking and manipulating small-ish samples.

  

  

* mitmproxy : 

  <img src="Image/mitmweb.jpg" alt="mitmweb" style="zoom:50%;" />

  > mitmproxy is a console tool that allows interactive examination and modification of HTTP traffic. It differs from mitmdump in that all flows are kept in memory, which means that it’s intended for taking and manipulating small-ish samples. 

  

* mitmdump : 

  > mitmdump is the command-line companion to mitmproxy. It provides tcpdump-like functionality to let you view, record, and programmatically transform HTTP traffic. See the `--help` flag output for complete documentation.

  > mitmprxoy的命令行接口，同时还可以对接Python对请求进行处理，有了它我们可以不用手动截获和分析HTTP请求和响应，值需要写好请求和响应的处理逻辑即可。它还可以实现数据的解析、存储等工作，这些过程都可以通过Python实现。

#### 软件安装

* [mitmproxy官方下载地址]([mitmproxy - an interactive HTTPS proxy](https://www.mitmproxy.org/))

#### 软件使用

1. 进入软件安装目录，打开`PowerShell`。或者`win+r`打开命令行

2. 键入`mitmweb`、`mitmproxy`、`mitmdump`命令

3. 可选参数:

   `-p` : 端口号(默认8080，如果冲突可以通过该参数修改)，从该参数指定的端口捕获发送自手机的请求

   `--no-server` : 

   `-s` : 指定文件名。指定该文件来处理截获的数据(文件路径需要在软件安装的路径)，常配合`mitmdump`命令执行。

   `-w` : 指定文件名。把截获的数据保存到文件中。

#### Python库安装

```python
pip install mitmproxy
```

#### Python库导入

```python
from mitmproxy import ctx
```



### 使用

```python
import json
import pymongo
from mitmproxy import ctx

client = pymongo.MongoClient('localhost')
db = client['igetget']
collection = db['books']

# 函数名只能写response或者request，分别对应处理发送自服务器的数据或者发送自手机的数据
# flow是HTTPFlow对象
def response(flow):
    global collection
    url = 'https://dedao.igetget.com/v3/discover/bookList'
    
    # 对捕获的请求url进行过滤
    if flow.request.url.startswith(url):
        text = flow.response.text
        data = json.loads(text)
        books = data.get('c').get('list')
        for book in books:
            data = {
                'title': book.get('operating_title'),
                'cover': book.get('cover'),
                'summary': book.get('other_share_summary'),
                'price': book.get('price')
            }
            ctx.log.info(str(data))
            collection.insert(data)
```



### 问题解决 

问题一：

> Client TLS handshake failed. The client may not trust the proxy's certificate

![4.5](Image/4.5.jpg)

* [报错原因]([(7条消息) Android 7.0 之后抓包 unknown 和证书无效的解决方案（无需改代码）_ShadowySpirits的博客-CSDN博客](https://blog.csdn.net/ShadowySpirits/article/details/79756274))
* [解决方法-Android开发者文档]([网络安全配置  | Android 开发者  | Android Developers (google.cn)](https://developer.android.google.cn/training/articles/security-config.html))

* [解决方法-CSDN]([(7条消息) Charles Android 抓包失败SSLHandshake: Received fatal alert: certificate_unknown_MrgcXia的博客-CSDN博客](https://blog.csdn.net/mrxiagc/article/details/75329629))

* [解决方法-CSDN]([(7条消息) Android 7.0 之后抓包 unknown 和证书无效的解决方案（无需改代码）_ShadowySpirits的博客-CSDN博客](https://blog.csdn.net/ShadowySpirits/article/details/79756274))



## Appium自动化检测

### 环境安装

* [环境安装](.\课件\08-appium的使用\1.appium环境安装.md)

### Appium使用

 #### 安装Python模块

````python
pip install appium-python-client
````



#### 建立adb server和模拟器的连接

1. 启动夜神模拟器，进入夜神模拟器所在的安装路径的bin目录下，进入cmd终端键入`adb devices`

```shell
C:\Program Files (x86)\Nox\bin>adb devices
List of devices attached
* daemon not running; starting now at tcp:5037
* daemon started successfully
```

2. `nox_adb.exe connect 127.0.0.1:62001`

```
C:\Program Files (x86)\Nox\bin>nox_adb.exe connect 127.0.0.1:62001
already connected to 127.0.0.1:62001
```

3. `adb devices`

```
C:\Program Files (x86)\Nox\bin>adb devices
List of devices attached
127.0.0.1:62001 device
```



#### 启动appium服务

启动appium-desktop，点击start server

```
[Appium] Welcome to Appium v1.10.0
[Appium] Appium REST http interface listener started on 0.0.0.0:4723
```



#### 获取Desired Capabilities参数

1. 获取模拟设备的型号`deviceName`
   - 打开设置——关于平板电脑
   - 查看型号，获取模拟设备的型号
2. 获取app包名称`appPackage` 以及 app进程名`appActivity`
   - 打开模拟器中的抖音短视频app
   - 在adb连接正确的情况下，在夜神模拟器安装目录的bin目录下的cmd中输入`adb shell`
   - 进入adb shell后输入 `dumpsys activity | grep mFocusedActivity`
   - `com.ss.android.ugc.aweme`就是app包名
   - `.main.MainActivity`就是进程名



#### 初始化以及获取设备分辨率

> 完成代码如下，并运行代码查看效果：如果模拟器中抖音app被启动，并打印出模拟设备的分辨率则成功

```python
from appium import webdriver

# 初始化配置，设置Desired Capabilities参数
desired_caps = {
    'platformName': 'Android',
    'deviceName': 'SM-G955F',
    'appPackage': 'com.ss.android.ugc.aweme',
    'appActivity': '.main.MainActivity'
}
# 指定Appium Server
server = 'http://localhost:4723/wd/hub'
# 新建一个driver
driver = webdriver.Remote(server, desired_caps)
# 获取模拟器/手机的分辨率(px)
width = driver.get_window_size()['width']
height = driver.get_window_size()['height']
print(width, height)
```

- 移动设备分辨率

  - driver.get_window_size()['width']
- driver.get_window_size()['height']




#### 定位元素以及提取文本的方法

点击appium desktop右上角的放大镜图标

> 根据Desired Capabilities填写配置，并点击start session

![appium-session配置](../爬虫开发笔记/课件/08-appium的使用/images/appium-session配置.png)

定位界面的使用方法如下图所示

![2-3-2-appiumDesktop定位界面使用方法](../爬虫开发笔记/课件/08-appium的使用/images/2-3-2-appiumDesktop定位界面使用方法.jpg)

点击短视频的作者名字，查看并获取该元素的id

![元素定位](../爬虫开发笔记/课件/08-appium的使用/images/元素定位.png)



通过元素id获取该元素的文本内容

> 实例化appium driver对象后添加如下代码，运行并查看效果

```
# 获取视频的各种信息：使用appium desktop定位元素
print(driver.find_element_by_id('bc').text)  # 发布者名字
print(driver.find_element_by_id('al9').text)  # 点赞数
print(driver.find_element_by_id('al_').text)  # 留言数
print(driver.find_element_by_id('a23').text)  # 视频名字，可能不存在，报错
```

- 定位元素及获取其文本内容的方法

  - driver.find_element_by_id(元素的id).text
  - driver.find_element_by_xpath(定位元素的xpath规则).text


#### 控制抖音app滑动

appium滑动的函数

- driver.swipe(start_x, start_y, end_x, end_y)

控制抖音app滑动的代码实现

```
start_x = width // 2  # 滑动的起始点的x坐标，屏幕宽度中心点
start_y = height // 3 * 2  # 滑动的起始点的y坐标，屏幕高度从上开始到下三分之二处
distance = height // 2  # y轴滑动距离：屏幕高度一半的距离
end_x = start_x # 滑动的终点的x坐标
end_y = start_y-distance # 滑动的终点的y坐标
# 滑动
driver.swipe(start_x, start_y, end_x, end_y)
```



#### 完整代码

```python
import time
from appium import webdriver


class DouyinAction():
    """自动滑动，并获取抖音短视频发布者的id"""
    def __init__(self, nums:int=None):
        # 初始化配置，设置Desired Capabilities参数
        self.desired_caps = {
            'platformName': 'Android',
            'deviceName': 'SM-G955F',
            'appPackage': 'com.ss.android.ugc.aweme',
            'appActivity': '.main.MainActivity'
        }
        # 指定Appium Server
        self.server = 'http://localhost:4723/wd/hub'
        # 新建一个driver
        self.driver = webdriver.Remote(self.server, self.desired_caps)
        # 获取模拟器/手机的分辨率(px)
        width = self.driver.get_window_size()['width']
        height = self.driver.get_window_size()['height']
        print(width, height)
        # 设置滑动初始坐标和滑动距离
        self.start_x = width//2 # 屏幕宽度中心点
        self.start_y = height//3*2 # 屏幕高度从上开始到下三分之二处
        self.distance = height//2 # 滑动距离：屏幕高度一半的距离
        # 设置滑动次数
        self.nums = nums

    def comments(self):
        # app开启之后点击一次屏幕，确保页面的展示
        time.sleep(2)
        self.driver.tap([(500, 1200)], 500)

    def scroll(self):
        # 无限滑动
        i = 0
        while True:
            # 模拟滑动
            print('滑动ing...')
            self.driver.swipe(self.start_x, self.start_y,
                              self.start_x, self.start_y-self.distance)
            time.sleep(1)
            self.get_infos() # 获取视频发布者的名字
            # 设置延时等待
            time.sleep(4)
            # 判断是否退出
            if self.nums is not None and self.nums == i:
                break
            i += 1

    def get_infos(self):
        # 获取视频的各种信息：使用appium desktop定位元素
        print(self.driver.find_element_by_id('bc').text) # 发布者名字
        print(self.driver.find_element_by_id('al9').text) # 点赞数
        print(self.driver.find_element_by_id('al_').text) # 留言数
        print(self.driver.find_element_by_id('a23').text) # 视频名字，可能不存在，报错

        # # 点击【分享】坐标位置 671,1058
        # self.driver.tap([(671, 1058)])
        # time.sleep(2)
        # # 向左滑动露出 【复制链接】 580，1100 --> 200, 1100
        # self.driver.swipe(580,1100, 20, 200, 1100)
        # # self.driver.get_screenshot_as_file('./a.png') # 截图
        # # 点击【复制链接】 距离右边60 距离底边170 720-60，1280-170
        # self.driver.tap([(660, 1110)])
        # # self.driver.get_screenshot_as_file('./b.png')  # 截图

    def main(self):
        self.comments() # 点击一次屏幕，确保页面的展示
        time.sleep(2)
        self.scroll() # 滑动


if __name__ == '__main__':

    action = DouyinAction(nums=5)
    action.main()
```



> 至此，可以参考爬虫5.0课程项目库，使用fiddler等抓包工具，利用appium+mitmproxy+wget等python模块自动获取抖音视频文件



### 关于模拟式移动端爬虫的参考阅读

1. https://zhuanlan.zhihu.com/appium  
2. https://github.com/butomo1989/docker-android
3. https://blog.csdn.net/weixin_42620645/article/details/83828863
4. https://blog.csdn.net/weixin_39211232/article/details/83410130#Android_16
5. https://www.jianshu.com/p/bf1ca3d4ac76
6. http://www.testclass.net/appium/





### webdriver.common package

#### .touch_action module

*class*`TouchAction`(*driver: Optional[WebDriver] = None*)

> `long_press`(*el: Optional[WebElement] = None*, *x: Optional[int] = None*, *y: Optional[int] = None*, *duration: int = 1000*) → T

* Begin a chain with a press down that lasts duration milliseconds
* `Params` :
  * **el** – the element to press
  * **x** – x coordiate to press. If y is used, x must also be set
  * **y** – y coordiate to press. If x is used, y must also be set
  * **duration** – Duration to press
* `Returns`  : Self instance
* `Return type` : TouchAction



> `move_to`(*el: Optional[WebElement] = None*, *x: Optional[int] = None*, *y: Optional[int] = None*) → T

* Move the pointer from the previous point to the element or point specified
* `Params` : 
  * **el** – the element to be moved to
  * **x** – x coordiate to be moved to. If y is used, x must also be set
  * **y** – y coordiate to be moved to. If x is used, y must also be set
* `Returns` : Self instance
* `Return type` : TouchAction



> `perform`() → T

* Perform the action by sending the commands to the server to be operated uponReturnsSelf instanceReturn typeTouchAction



> `press`(*el: Optional[WebElement] = None*, *x: Optional[int] = None*, *y: Optional[int] = None*, *pressure: Optional[float] = None*) → T

* Begin a chain with a press down action at a particular element or point
* `Params` :
  * **el** – the element to press
  * **x** – x coordiate to press. If y is used, x must also be set
  * **y** – y coordiate to press. If x is used, y must also be set
  * **pressure** – [iOS Only] press as force touch. Read the description of force property on Apple’s UITouch class for more details on possible value ranges.
* `Returns` : Self instance
* `Return type` : TouchAction



> `release`() → T

* End the action by lifting the pointer off the screenReturnsSelf instanceReturn typeTouchAction



> `tap`(*element: Optional[WebElement] = None*, *x: Optional[int] = None*, *y: Optional[int] = None*, *count: int = 1*) → T

* Perform a tap action on the element
* `Params` : 
  * **element** – the element to tap
  * **x** – x coordinate to tap, relative to the top left corner of the element.
  * **y** – y coordinate. If y is used, x must also be set, and vice versa
* `Returns` : Self instance
* `Return type` : TouchAction



> `wait`(*ms: int = 0*) → T

* Pause for ms milliseconds.
* `Params` : 
  * **ms** – The time to pause
* `Returns` : Self instance
* `Return type` : TouchAction



#### .mobileby module

*class*`MobileBy`

Bases: `selenium.webdriver.common.by.By`

`ACCESSIBILITY_ID`*= 'accessibility id'*

`ANDROID_DATA_MATCHER`*= '-android datamatcher'*

`ANDROID_UIAUTOMATOR`*= '-android uiautomator'*

`ANDROID_VIEWTAG`*= '-android viewtag'*

`ANDROID_VIEW_MATCHER`*= '-android viewmatcher'*

`CUSTOM`*= '-custom'*

`IMAGE`*= '-image'*

`IOS_CLASS_CHAIN`*= '-ios class chain'*

`IOS_PREDICATE`*= '-ios predicate string'*

`IOS_UIAUTOMATION`*= '-ios uiautomation'*

`WINDOWS_UI_AUTOMATION`*= '-windows uiautomation'*



#### .multi_action module

class*`MultiAction`(*driver: WebDriver*, *element: Optional[WebElement] = None*)

> `add`(**touch_actions: TouchAction*) → None

* Add TouchAction objects to the MultiAction, to be performed later.

* `Params` : 

  * **touch_actions** – one or more TouchAction objects describing a chain of actions to be performed by one finger

* Usage:

  ```python
  a1 = TouchAction(driver)a1.press(el1).move_to(el2).release()a2 = TouchAction(driver)a2.press(el2).move_to(el1).release()MultiAction(driver).add(a1, a2).perform()
  ```

* `Returns` : Self instance

* `Return type` : MultiAction



> perform`() → TPerform the actions stored in the object.

* `Returns` : Self instance
* `Return type` : MultiAction



### webdriver.extensions package

#### .action_helpers module

*class*`ActionHelpers`**(**command_executor='http://127.0.0.1:4444/wd/hub'**,** *desired_capabilities=None***,** *browser_profile=None***,** *proxy=None***,** *keep_alive=False***,** *file_detector=None***,** *options=None***)**



>`drag_and_drop`**(**origin_el: appium.webdriver.webelement.WebElement**,** *destination_el: appium.webdriver.webelement.WebElement***)** **→** **T**

* Drag the origin element to the destination element

* `params` : 
  * **origin_el** – the element to drag
  * **destination_el** – the element to drag to
* `Returns` : Self instance
* `Return type` : Union[‘WebDriver’, ‘ActionHelpers’]



> `flick`**(**start_x: int**,** *start_y: int***,** *end_x: int***,** *end_y: int***)** **→** **T**

* Flick from one point to another point.

* `params` : 

  * **start_x** – x-coordinate at which to start
  * **start_y** – y-coordinate at which to start
  * **end_x** – x-coordinate at which to stop
  * **end_y** – y-coordinate at which to stop

* `usage` :

  ```python
  driver.flick(100, 100, 100, 400)
  ```

* `Returns` : Self instance

* `Return type` : Union[‘WebDriver’, ‘ActionHelpers’]



> `scroll`**(**origin_el: appium.webdriver.webelement.WebElement**,** *destination_el: appium.webdriver.webelement.WebElement***,** *duration: Optional[int] = None***)** **→** **T**

* Scrolls from one element to another
* `params` :
  * **origin_el** – the element from which to being scrolling
  * **destination_el** – the element to scroll to
  * **duration** – a duration after pressing originalEl and move the element to destinationEl. Default is 600 ms for W3C spec. Zero for MJSONWP.
* `Returns` : Self instance
* `Return type` : Union[‘WebDriver’, ‘ActionHelpers’]



> `swipe`**(***start_x: int***,** *start_y: int***,** *end_x: int***,** *end_y: int***,** *duration: int = 0***)** **→** **T**

* Swipe from one point to another point, for an optional duration.

* `params` : 

  `params` :

  * **start_x** – x-coordinate at which to start
  * **start_y** – y-coordinate at which to start
  * **end_x** – x-coordinate at which to stop
  * **end_y** – y-coordinate at which to stop
  * **duration** – time to take the swipe, in ms.

* `Returns` : Self instance

* `Return type` : Union[‘WebDriver’, ‘ActionHelpers’]



> `tap`**(**positions: List[Tuple[int, int]]**,** *duration: Optional[int] = None***)** **→** **T**

* Taps on an particular place with up to five fingers, holding for a certain time
* `params` : 
  * **positions** – an array of tuples representing the x/y coordinates of the fingers to tap. Length can be up to five.
  * **duration** – length of time to tap, in ms
* `Returns` : Self instance
* `Return type` : Union[‘WebDriver’, ‘ActionHelpers’]



### [官方文档]([webdriver package — Python client 1.0 1.0 documentation (appium.github.io)](https://appium.github.io/python-client-sphinx/webdriver.html#))



